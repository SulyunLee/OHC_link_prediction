'''
Author: Sulyun Lee
This script generates the topic distribution of all posts using LDA.
'''

#--------------------Import libraries--------------------
import pandas as pd
import numpy as np
import re
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS
import nltk
from nltk.stem import WordNetLemmatizer, SnowballStemmer
from nltk.stem.porter import *
#--------------------------------------------------------

# download package
# nltk.download('wordnet')

def lemmatize_stemming(text):
    '''
    This function is for lemmatizing and stemming a text.
    It uses nltk.stem.porter package for stemming and nltk.stem package for lemmatizing.
    Input:
      - text: the string type with text to lemmatize and stem.
    Output:
      - processed text
    '''
    stemmer = PorterStemmer()
    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))

def text_preprocess(text):
    '''
    This function preprocesses text by tokenizing, removing stop words, and extracting
    only useful words.
    Input:
      - text: the string type with text to preprocess.
    Output:
      - result: the list which includes words after tokenizing and preprocessing.
    '''
    result = []
    for token in simple_preprocess(text):
        if token not in STOPWORDS and len(token) > 3: # remove stopwords and keep tokens length over 3.
            result.append(lemmatize_stemming(token))
    
    return result

def generate_lda_topic_distr(active_ntwk_df, text_preprocess):
    '''
    This function generates the topic distribution using LDA.
    Input:
      - active_ntwk_df: the dataframe which includes the column 'Message'.
                        This column should include the texts to generate LDA.
      - text_preprocess: the function to be used for preprocessing texts in active_ntwk_df texts.
    Output:
      - lda_arr: the numpy array which includes the topic distribution for all posts in active_ntwk_df.
    '''

    print('Text preprocessing...')
    active_ntwk_df.loc[active_ntwk_df['Message'].isna(), 'Message'] = "" # put empty string to NA rows.
    processed_text = active_ntwk_df['Message'].map(text_preprocess) # preprocess texts

    print('Generating dictionaries and doc2bow...')
    # generate bag of words
    dictionary = gensim.corpora.Dictionary(processed_text)

    # filter out tokens
    # - less than 10 documents
    dictionary.filter_extremes(no_below=10)

    # doc2bow
    bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]

    # model LDA
    print('Fitting LDA model...')
    topic_num = 30 # specify the number of topics to consider
    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=topic_num, id2word=dictionary, passes=2, workers=2)

    # create the vector array for each corpus
    print('Generating topic distribution array')
    lda_arr = np.zeros((processed_text.shape[0], topic_num))
    for idx in range(len(bow_corpus)): #for every corpus
        topic_distr = lda_model.get_document_topics(bow_corpus[idx])
        for t in topic_distr:
            lda_arr[idx,t[0]] = t[1]

    return lda_arr

def generate_lda_df(start_week, end_week):
    '''
    This function generates the dataframe that includes the useful columns and topic distributions.
    Input: 
      - start_week: the integer type of number for the starting week
      - end_week: the integer type of number of the ending week
    Output:
      - user_lda_df: The dataframe that includes the columns of source, channel type, week number,
                     and topic distributions generated by LDA model.
    '''

    # the aggregated dataframe with messages from three channels included
    three_ntwk_file = '../dataset/three_network_withmsg.csv'
    
    print('Reading data file...')
    three_ntwk_edgelist = pd.read_csv(three_ntwk_file)
    three_ntwk_edgelist['Time'] = pd.to_datetime(three_ntwk_edgelist['Time'])

    # extract only the used time period (week 50-69)
    weeks = [g for n, g in three_ntwk_edgelist.groupby(pd.Grouper(key='Time', freq='W'))]
    weekly_group = weeks[start_week:end_week+1]
    active_ntwk_df = pd.concat(weekly_group, axis=0)
    active_ntwk_df = active_ntwk_df.reset_index(drop=True)
    weekly_group = active_ntwk_df.groupby(pd.Grouper(key='Time', freq='W'))

    print('Making week column...')
    # make the Week column into the active_ntwk_df which indicates the number of weeks for each post
    week = 50
    active_ntwk_df['Week'] = 0
    for i, g in weekly_group:
        g_idx= np.array(g.index)
        active_ntwk_df.loc[g_idx, 'Week'] = week
        week += 1

    # generate LDA topic distribution array by calling the function 
    lda_arr = generate_lda_topic_distr(active_ntwk_df, text_preprocess)

    # construct the dataframe to return
    # this dataframe includes the columns source, Type, Week, and topic distr.
    user_lda_df = active_ntwk_df[['source','Type','Week']]
    user_lda_df['topic_distr'] = lda_arr.tolist() # this column includes the lists

    return user_lda_df





    

